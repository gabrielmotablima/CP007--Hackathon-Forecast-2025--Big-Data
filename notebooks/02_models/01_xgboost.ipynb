{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "703f0c27",
   "metadata": {},
   "source": [
    "# LOADING LIBS AND MODULES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2285e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yuric\\OneDrive\\Área de Trabalho\\HACKATON\\CP007--Hackathon-Forecast-2025--Big-Data\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# core\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import os\n",
    "import yaml\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# import shap\n",
    "\n",
    "# modeling\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, TimeSeriesSplit\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer, make_column_selector\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, FunctionTransformer\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "\n",
    "# models\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# otm\n",
    "import optuna\n",
    "\n",
    "# our functions\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7de57a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21530e28",
   "metadata": {},
   "source": [
    "# LOADING THE PARAMETER CONFIGURATION FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1eacba91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading up the YAML dictionary that contains all the parameters for the model\n",
    "with open(\"model_params.yml\", \"r\") as f:\n",
    "    configs = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ede82d",
   "metadata": {},
   "source": [
    "# SETTING UP DEFAULTS AND PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e74cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# showing all columns in a pandas dataframe\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "# extracting the keys associated with the parameters\n",
    "RANDOM_STATE = configs['RANDOM_STATE']\n",
    "NUMBER_OF_FOLDS = configs['NUMBER_OF_FOLDS']\n",
    "STRATIFICATION = configs['STRATIFICATION']\n",
    "FEATURE_STRATIFICATION = configs['FEATURE_STRATIFICATION']\n",
    "EVAL_FEATURES = configs['EVAL_FEATURES']\n",
    "\n",
    "FEATURE_COLUMNS = configs['FEATURES']\n",
    "LABEL_COLUMNS = configs['TARGET']\n",
    "\n",
    "PREPROCESSING = configs['PREPROCESSING']\n",
    "\n",
    "# OPTUNA AND MODEL PARAMS\n",
    "OPTUNA_PARAMS = configs['OPTUNA_PARAMS']\n",
    "MODEL_RANGE_PARAMS = OPTUNA_PARAMS['MODEL']\n",
    "\n",
    "# SHAP PARAMS\n",
    "SHAP_PARMS = configs['SHAP_PARAMS']\n",
    "SHAP_SAMPLE = SHAP_PARMS['SHAP_SAMPLE']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ac4e26",
   "metadata": {},
   "source": [
    "# LOADING THE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef02f5ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the dataset containing the training data for the demand forecasting model.\n",
      "The dataset contains 6227874 rows and 25 columns.\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading the dataset containing the training data for the demand forecasting model.\")\n",
    "df = pd.read_parquet('../../data/processed/processed_data.parquet')\n",
    "df = df.sort_values('week_of_year').reset_index(drop=True)\n",
    "print(f\"The dataset contains {df.shape[0]} rows and {df.shape[1]} columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e90e66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering\n",
    "df = df[~((df.week_of_year == 37) & (df.quantity > df[df.week_of_year == 37].quantity.quantile(0.90)))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3340ad6",
   "metadata": {},
   "source": [
    "### Fill NaN with -1, indicating a new release in this city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41d33231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lista de colunas onde quer aplicar o preenchimento\n",
    "cols_to_fill_month = [\"previous_month_quantity_mean\", \"previous_month_gross_value_mean\", \"previous_month_net_value_mean\", \"previous_month_gross_profit_mean\", \"previous_month_discount_mean\"]\n",
    "\n",
    "# condição: linhas onde month != 1\n",
    "mask_month = df[\"month\"] != 1\n",
    "\n",
    "# aplica apenas nessas linhas\n",
    "df.loc[mask_month, cols_to_fill_month] = df.loc[mask_month, cols_to_fill_month].fillna(-1)\n",
    "\n",
    "# lista de colunas onde quer aplicar o preenchimento\n",
    "cols_to_fill_week = [\"previous_week_quantity_mean\", \"previous_week_gross_value_mean\", \"previous_week_net_value_mean\", \"previous_week_gross_profit_mean\", \"previous_week_discount_mean\"]\n",
    "\n",
    "# condição: linhas onde month != 1\n",
    "mask_week = df[\"week_of_year\"] != 1\n",
    "\n",
    "# aplica apenas nessas linhas\n",
    "df.loc[mask_week, cols_to_fill_week] = df.loc[mask_week, cols_to_fill_week].fillna(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7252a21",
   "metadata": {},
   "source": [
    "# SEPARATING THE FEATURES BY TYPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffe99bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping which columns should be treated as numerical columns - excluding cyclical features\n",
    "NUMERIC_COLUMNS = [column for column in df[FEATURE_COLUMNS].select_dtypes(np.number).columns if column not in PREPROCESSING['CYCLICAL_FEATURES'].keys()]\n",
    "# mapping which columns should be treated as categorical columns\n",
    "CATEGORICAL_COLUMNS = [column for column in df[FEATURE_COLUMNS].select_dtypes('object').columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a51143d",
   "metadata": {},
   "source": [
    "# SPLITTING THE DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa274d5",
   "metadata": {},
   "source": [
    "### Mapping which instances should be placed in the train and test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf4e6d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the data into training and testing sets\n",
    "# training set: all data before December 2022\n",
    "X_train = df.loc[df.month != 12, FEATURE_COLUMNS]\n",
    "y_train = df.loc[df.month != 12, [LABEL_COLUMNS]]\n",
    "# test set: all data from December 2022 onwards\n",
    "X_test = df.loc[df.month == 12, FEATURE_COLUMNS]\n",
    "y_test = df.loc[df.month == 12, [LABEL_COLUMNS]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650c722c",
   "metadata": {},
   "source": [
    "### Creating the cross validation folds"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAAS8AAACYCAYAAAC1Zm+hAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAJcEhZcwAADsMAAA7DAcdvqGQAACDTSURBVHhe7d15XJTl/v/x1zADiCzBUSE3zIqEXI9LavqNShSXk0WLKemv0jpqWqaQu+J+jnJMSlvdzQVTw0OaiOiRSkEQM2WNCkLFBC2BYZ9hfn+kc5xbBStmmDl8no8HPobr+tz3oKNvr/u+r/u6VdnZ2QaEEMLG2CkbhBDCFkh4CSFskoSXEMImSXgJIWyShJcQwiZJeAkhbJKElxDCJkl4CSFskoSXEMImqWSGvXmcP3+eo0ePKptRq9UMHjyYY8eO8be//U3Z/YclJCSQk5ODi4sLPj4++Pr6olKplGW3lZ+fz08//UTfvn2VXXckMzOTM2fOoNFoaNeuHd26dUOtVivLjH799VeSkpIIDAzkl19+ITk5mcDAQGWZELclIy8zqampQafTodPpWLlyJV9//bXx++Li4lsG2x8VGhrKzJkz+eWXX0hPT2fixIlUV1cry2pVUFDA6dOnlc13JDIykqeeeoqcnBzOnTvH4sWLKSwsVJaZOHfuHOHh4QAUFRWRkJAAwOHDhxk8eLCiWoibycjLAoYOHUpwcDCjR48GoLi4mFOnTvHoo49SXFxMRkYGLi4u5OXl0bVrV7y8vEhMTKSsrIxHH30Ue3t7AHQ6HcePH0er1dK5c2fatm0LgI+PD6dPn8bZ2dnkfa9vk5iYSElJCR07dsTb2xuAo0eP0qVLF9LS0nBycsLb25v8/Hy6desGwOXLlzl9+jQqlYpevXrh5uYG10aU6enpqFQqOnTogLe3N0OGDGH06NG88MILJu8NEBMTQ9++fUlKSsLZ2ZmHHnoIjUbDmTNnmDZtGnFxcRQVFZGamkqvXr0IDw9n7969hIWF0bJlS3r06KHcpRAgI6+GkZubS0hIiPH1iy++SHh4OAkJCfj7+zNv3jwOHDjApk2bGDBgAFwLod69e7NlyxbOnDnD448/zqVLl4z73LRpEzk5OdTU1BjbAB555BHWr1/PqVOnGDBgAJmZmQCEhIQwbNgwjhw5QmZmJt9++y0ffPABXDuE9Pf359ChQ3z++ec8+uijlJSUcOnSJQICAjhx4gSJiYmsW7cOgGbNmrFv3z7S09OpqKgwef/XX3+dl19+mW+++YawsDAmTZpk0g+Qk5NDWFgYNTU1FBYWUllZyblz57hy5YqyVAgjCS8r0LRpUzZt2sSiRYto2bIl7u7uLFmyhI8++oiLFy8CcPr0aVq2bMm6deuYOXMmcXFxxpFcXFwcUVFRDBo0iA4dOvDJJ59QU1PD8ePH8fLyYsOGDcyZM4fdu3czZ84c4/tOmzaNsLAw436ue+utt1i+fDnLly8nIiKChx9+mEOHDvHdd9/Ro0cPQkJCmDdvHosWLQLgnXfeAeDJJ5+kc+fOzJ07l6qqKpP9TZ8+naioKI4cOUJJSYmx70ZNmjThiSeeoFWrVkycOJFBgwYpS4QwkvCyAu3atTO+btu2LY8++iiA8XARIDY2lqysLDp06ECHDh0YNGgQFy5cMG4fFxdHdnY269atY9GiReTn5xMXF0dqaiq+vr506NCBESNGcPnyZeM+fXx8jK9vlJeXx9SpU43vdejQIYqLi/m///s/PDw86NmzJ0OHDuXEiRNwbeS1Y8cOsrOz2b9/P3v37mXz5s3G/XXv3h0AFxcXPD09+eGHH4x9QvxREl5WQHlVUPk9wIMPPsigQYPIysoyfqWnpwNgMPz3tKW/vz8AV65cwdfXl2HDhplsEx8fb6y1s7v1x+/h4cGGDRtMtnvppZcAWLNmDenp6SxfvpzRo0ej0+lM3v+BBx7gySefJC8vz9iWm5sLQHV1NQUFBdx9993GPiH+qFv/7RVWZ8iQIcTGxnLw4EEyMzNJSkoiMjISgFGjRpGYmEhmZib79u0DwNvbmxEjRrB//36io6PJyMjgxIkTREdHK/Z8s9DQUKZOncrx48dJTU3l3//+N1lZWSQkJHDgwAEyMjI4d+4cXAvAkJAQvvjiCzIyMoiPj+fTTz/lscceM+7vn//8J2lpaYSHh3PPPffUGl533XUXhYWFZGZmGg+ZhbgVCS8L6Ny5M15eXsbvXVxc6Nmzp/F1x44djX1+fn64urrCtWDw9fUFwNHRkdTUVGJjY3nrrbfYtWsXAQEBAEyZMoVt27YRGhrKoUOHSElJwcPDA4D09HSOHz/OjBkz2L17t/HqXc+ePXFxcTG+r4eHBw888ABcO8n/ySefsG7dOhYuXEhubi5t27blvvvuIzExkTlz5nDo0CESEhKws7Nj/PjxJCQkMGfOHLZs2cKuXbt4/PHHjfuePXs2S5cu5ZdffjGGq4uLC127dr3pdY8ePXjxxRcJCQlh06ZNxn0IoSRTJYRZ+fj4kJ2drWxuFAwGg8khdWOkUqlueRqkPkh4CbN69tln2b17t7L5f57BYODChQvo9XplV6NhMBhwc3PDw8PDLAEm4SWEGeh0Oq5cuWJyuqAxysnJoV27dre9OPRn2Ex4VVVVkZycbJY/BCHqQ0VFBY888ghqtRqdTsfVq1dp3ry5sqzBFBYW0qJFC2WzWf3000+0bdvWLP9ubSa8tFotmZmZ+Pn5KbuEsAovvPACu3btwt7e3iS89Jcvoi/IV5bXSaXWoPHphMrO9Ab3Xr16UV1dTWlpKe7u7kyaNMk4lUXp+t0Tzz//PBUVFTRp0kRZ8qcEBAQQFRVlvMikJOF1Lbx++uknkytzQliTZ555hsjIyJvCq/TzzVRnnFKW3xHX8WGo7/qLspnz588zf/58Pv74Y1asWMEXX3zBc889x6RJk5g8eTJnz54lODiY+Ph4vvnmG/z8/Bg4cCCvv/46c+bMoVmzZuzZs4dt27bRtm1bnn/+ebRaLWVlZXz55ZfG9zl48CCLFy+mefPm7N69m7179xIREYGvry8ffvih8ep4ZGSk8Wr1jcwZXvW/RyGExSQkJODo6MiHH35ITk4OhYWFXLhwgV27dvHaa6+xaNEilixZYjK/T6/XM3HiRA4fPsyECRM4d+4cEydO5MCBAzfNrVu4cCHr169n586daDQaoqKi+PDDDwkMDCQrK4t27doRHx9/y+AyNwkvIWxYaWkpHh4euLu7ExoaipeXF1u2bGHt2rXGVT6U0zU0Gg1OTk7Y2dnx66+/cvnyZe655x4A4+oh1x0/fpwzZ87QqVMnqqqqcHNzw93dnb59+3L//ffDLfZvKRJeQtiwXr16sX79euN9pYWFhezZs4eHHnqI0tJSmjZtSnx8vPFWslvp1KkTY8aMMd7DeqOZM2fSpk0b7r77bgwGAykpKaSkpHDs2DHKysro3LkzSUlJlJeXm2xnCXLOS4h6crtzXuXxn1OVlqwsr5vKDteXZ2DXxEnZU68MBgPl5eWo1Wo6depUr5OKzXnOS8JLiHpyu/Cydnq9nueee45Lly6xcePGej1/JeF1LbwmLE7lL16/3etnaX2KYuhTfBAHQ6Wyy/IcHQFos3yjskc0IFsNL3MyZ3jV/x7/hJKSErRarbLZKqjRW0dwAVRW/vYlRCNmNeG1bds2Pv/8c8LDw2WxOiFEnawmvBYsWEBwcDALFy5kwoQJxvaSkhLOnz9vXDVUCFtjMBjQ6//AV82tz+gUFRWZfFXWMgoPDg5WNgGQlJRERkaGsvlPqaiosOiN6FZzzuvGpVO6d+/OqVO/zUiuqqqisrISrVbLWxE/Ndg5r35F++lXtF/Z3KDaRGxXNokGdLtzXu+uz+XA0f8uv32n7FSw471uuLlqTNoLCgrIz89nxYoVrFq1CgBnZ2c0Gg16vR69Xk/Tpk3RaDSUlpbi7OxMSUkJarUavV6Pq6srOp0OlUplnKNVWlqKi4sLarWayspKKioqcHR0xNHR0bgihMFgoKSkBJVKhYuLCwaDAa1Wi1qtpmnTpsyfP58RI0bQoUMHHBwcoLGc82rfvj1VVVVUVFTw9NNPG9sdHBxwdXW97b1TQli7skrTJzrdqRoDlFfcvK2npyfNmzenSZMmuLm50b9/f1JSUrh69SrR0dHk5eXRp08fAOPDVXr27El0dDTvvPMO2dnZJCcnk5mZyaFDh3j22WfJzc1l+PDhcG3e15UrV+jcuTOlpaXG9509ezbp6emkpqZSUVHBkCFDyMrKYsmSJZw5c4aamhqqqqpueoKVuVhNeMXGxtK1a1cCAgKYO3euslsIcRuenp74+/vj5eXFqVOnCAoKoqioyKTmrrvuYuTIkYwdO5ZDhw6Z9K1fv56OHTvy3XffwbVbgu69916eeeYZk7qBAwfyxhtvUFFRgZOTEz/++CPBwcHGex6dnJyMoWoJVnPYWBeZ5yWs3e0OG//5/o/EJ/yiLL8jm97ugleL3w7BbnT9xuz33nuPgQMH8vXXX5OWlkZsbCxvvvkmffr04cSJEwQFBREVFUXv3r05ceIEFy9e5LPPPqN79+64ubmRl5dHv379cHZ2xtfXl+zsbEJCQli5ciUDBw4kKirKZLlwgJ07d/Lwww8zZMgQzpw5Yzwk/Mc//sGoUaOMtxrRWA4bhRB/TocOHYiJiaFnz5506tRJ2X3HWrRogY+Pj/Hp6ddFRETg4+PDjh07aNmyJUePHsXPzw9fX1+ysrKYMGECAwcOtNizB2TkJUQ9ud3IS1uq4+fC/z6E907Za1S0a2PeW4NuZfXq1QQGBjJs2DAyMjLQaEwvGPwe5hx5SXgJUU9uF16NmTnDq/73KIQQFiDhJYSZ6HQ6ZVOjY85pE3LYKEQ9ufGw0WAwkJ+fT1lZmbKs0TAYDHh5eeHm5ta4H30m4SWs3Y3hRQOuMGptzBFcyGGjEOZz/WnRjf3LXGxq5DVzVRotWjbuR5+9fHExqEBTU4n5/lrUTaW2B7Uat4DhNO3ZX9ndKClHXsK8bCq8GnIxQmsxPe81ZVODcg8ag4v/EGVzoyThZVlWc9h46NAhNm7cyL59+5RdQghxE6sJr4EDB/Lyyy8rm6murkar1VrtCqtCiIZhNeF1OwUFBZw9e5a0tDRllxCiEbP68GrdujV9+/ald+/eyi4hRCNmNeF18uRJunXrxtSpU5k/f76yWwghTNjU1cbk0zn4PNC4p0p4Vl8EQGUw320Xd0SlApUKu7vcUTvLKrfI1UaLqzO8Ll26xNNPP01lZSUqlQp/f3/+9a9/KcvMTmbYC2sn4WVZdR42xsfHs3btWk6ePElycnKDBJcQQijVGV4dO3YkIiKCY8eOkZiYSFZWlrJECCEsrs7wat68OWPGjKGmpobq6mqLPpdNCCFup87w8vLyorS0lI0bN5KRkcGDDz6oLBFCCIurM7wOHz7Md999x+LFi2nbti3Lli1TlgghhMXVGV5FRUX06dOH1q1b061bN86fP68sEUIIi6szvAYNGsSOHTsYPHgws2bNYsGCBcoSIYSwuDrneVkLrVbLqk2ZtPRu3JNUrcXfCjdghx47Q02DriuGw28PZG32/yYreyxO5nlZVq3hFRkZiY+PD3v27DG2+fn5MWbMGJO6+jB79mwuX77MggULaNWqlbJb1vOyMta2rlibiO3KJouT8LKsWg8bR44cSVVVFfPmzWPZsmUsWrSI0tJSZVm9eOWVV1i+fDn+/v7KLiGEuEmt4XXx4kX279/P4cOHOXLkCNHR0WZbmubee+/F3d2dpk2bmrTrdDoqKiqoqKgwaRdCNG61hld1dTX29vZotVqKi4txdHRk6dKlyrJ6M2DAABISEkzaUlNT2bp1K5GRkSbtQojGrdbw8vb2JiwsjL59+6LX6ykrK+Ps2bPKsnrRr18/tmzZctODOrt168Yrr7zCSy+9ZNIuhGjcag0vgKSkJN577z3effddsrKyyM3NVZbUi4kTJ3L06FGio6OVXUIIcZNarzYC7Nq1i379+hESEsJHH31ESEgIa9euVZaZnVarZXdMNve0l9uTrMFfS+NRGWqwo9a/Puan1gDg0n+gssfi5GqjZdUZXj/88ANarZaUlBRiYmKYMGECjz/+uLLM7GQ9L2HtJLwsq87Dxvz8fLy9vRk7diyffvppgwSXEEIo1Rlebm5ubN26Fa1WS2VlJdXV1coSIYSwuDrDq7q6mtzcXMLCwpg9ezZbt25VlgghhMXVGV49e/bkpZdeIjAwkLCwMJ544glliRBCWFyd4XXy5Em2bdvGRx99RFlZGeHh4coSIYSwuDrDKzc3l/Hjx2NnZ4e9vT1FRUXKEiGEsLg6w+uRRx5h0aJFZGZmMnnyZEJDQ5UlQghhcXXO8zp//jyenp5cvnyZFi1a8O2339KzZ09lmdnJkjhCyU33C69eXIDaYHpLWYPQaBj/RQJ7v02XeV4WUuvIS6/XExcXR3FxMc2bN+fq1ausWbNGWVYvIiMjWbt2LXl5ecouIW5JhcE6ggtAp4MGfoh5Y1NreB0/fpwNGzYwadIkxo0bx6xZs5g2bZqyrF5069aNp556igEDBii7hBDiJrUeNmq1WmpqaqiqqjK2OTo64urqalJXH65cuUJkZCTHjh1j+/b/roqZlJTEV199RWVlJelFj8thozC6S3eF8fnzlM0NZnxMMnvPymGjpdQaXuvXr6djx45s2bLF2Na1a1fGjx9vUlcfdDodP//8M8OHD+fUqVMm7Xq9npKSEt4M/17CSxhJeDVutYaXJRUUFODg4EBAQAAnT55UdssJe3ETCa/GrdZzXpYUERHB4sWLTUZ5QtSlBrWyqWGo1TTsY5Qan1pHXrm5uVRXV+Pj46PssjhZEkdYO1kSx7JqHXlpNBreffddRo4cyZ49e2R2vRDCatQaXm3atGH16tVs27YNBwcHnnrqKd555x1lmRBCWFyt4QVQWVlJUlISMTEx/OUvf6Fv377KEiGEsLhawysxMZEnn3ySxMREVq1axZ49e3jooYeUZUIIYXG1hlfXrl15++23mTp1Kg4ODvz888+UlJQoy4QQwuJqDa+vvvoKrVZr/F6v17Nq1SqTGiGEaAi1htfVq1e5++67jd+7uLiY7bmNQgjxe9QaXp06dWLZsmXk5+dTUFDAtm3bePrpp5VlQghhcbVOUgWIj49n4cKF6PV6XnzxRcaOHassqVVxcTEajYamTZsqu34XrVbLx7HJtL3vAWVXnXqXHaCl7kfs0Nn8JOjYzUcAFXq9XtklGpDaoQnL/lPI4TS5PchSah15Afj7+3PkyBHi4+N/d3ABpKWlNfgaXXboUf8PBBeAXl8jwWWF9FUVGGodBoj6Vmd4/RFJSUkMGTKEF154wdhWWFhIUFAQQUFBXL58mcrKSp544gmGDh3KuXPn2LVrF4GBgYwcOdJkX0IIcSt1Hjb+Xjqdju7du3PmzBkAEhIS8PDwwMfHhx9//JGioiLWrFnDkiVL+PLLLwkMDKRZs2Z06tSJ6Oho2rRpg4ODg3F/Fy5cIC8vj7KyMr4tsvtDh419y/bRSveDstkmHdgQp2wSVuKfX5dwJF0OGy2l3kdeer0ejUajbOa9994jMzMTd3d3Lly4QJs2bejWrRuhoaHExsaSmppKUlLSTTdee3p60qVLFzp16mTSLoRo3Oo9vBwdHRkxYgTvv/8+O3fuNLY7ODhw5coVNmzYAEB2djZpaWn07dsXrVbLG2+8QbNmzWjRosUNewN7e3ucnZ1xdnY2aRdCNG71fthoLn/mamPfsv201P0AGGz+pL0cNloplR3//KpIDhstyKbCS9bzEtZM1vOyrHo/bBRCCEuQ8BJC2CQJLyGETZLwEkLYJAkvIYRNkvASQtgkCS8hhE2S8BJC2CSbmqT6R2fY30r0qQucOfcruhoDNbf5E1g6JguNneG3hyHXMTX/7p/t6ZjRBJUB7Ax1FN/g0oqVv+28ukrZ9bt5vTcMleNvP6yqrh/YzH6u7MLPVV0xYI/yUdKR7yVSU2OgRn+bP/g/oYmzgQXbwMGp7s/sTsVM/Ah9pY7a1rxRN2nKssOXZD0vCzL7yMtQywfekHQ1Bqr0tw8ugCYOBjSaO/tHYGcAdY3qdwUXALrqegkuADsne1R2dg0eXAAG1BhwuCm4AHTVNWYJLvjts3Jsemef2Z3SV1TXGlwA+oqyukpEPau38Prmm2/w8fFh6dKlRERE0K9fP55//nlmz57Npk2b8PHxYePGjQD4+PjAtVVWP/30U/Ly8ti9ezc+Pj589dVXij0LIcTN6iW8dDodEydOJDs7mwEDBgBQXl7Ozp07CQkJ4euvvyY7O5uzZ88qNzXKy8sjOzubcePGmbRXVVVRUlJCcXGxSbsQonGrl/CqrKzEyckJgGbNmgEYl7Cprq6me/fucO2BHjcuYXzjIWXv3r0BjLVCCFGbegkvZ2dnWrVqxfHjxwkJCTHpa9asGatWrSI1NZV//OMfqNVq+vTpwzfffMOIESOMdePGjeO7774jJSXFZHsHBwdcXV1xc3MzaRdCNG71El4AmzdvpmPHjsyePZv27dtz4MABuBY+aWlptG3blrS0NAA2bdrEvffey759+wgKCgJgx44deHl5kZGRYbJfIYS4lXqbKrFnzx5mzpyJv78/H3/8MXZ2d56LeXl5XL16lS5duii7jLRaLT/k5PKg34PKrj+kuqYGfY2y1VQT+98OcVXGX2pRo0Jdx/5uxVBZee3Fn/8YVE1vvERf1w9sXgbU1Nzm/0ZdpR4D8Nsv9ev61cbrr+uDrryy7p9VpWJE8Gh2frpLpkpYSL2Fl7nJYoTC2slihJZ16/8ahRDCykl4CSFskoSXEMImSXgJIWyShJcQwibZzNXGkpISsrKy8PX1VXYJYRVGjx7Nrl0yVcJSbCa8ioqKiI+P57777lN2WdTmzZsZOXIkjo6Oyi6Leuqpp9i7d6+y2aIqKyuJjIzkxRdfVHZZ1A8//ECrVq2Mt6g1lKlTp7Jv3z4cHByUXcIMbCa8rGWe18qVK5k8eXKDh5ePjw/Z2dnKZouqrKxkzZo1N90SZmlpaWm0a9cOFxcXZZdFyTwvy7KZc1729vZ4eHgomy2uS5cuqNVqZbPFDRkyRNlkcWq1uta7IizFw8PDKgIjMDDQKtZSayxsZuQlhBA3spmRlxBC3MgmRl7Hjh3j888/p7y8nL///e8WPe8VFRVFQUEB58+fx8/Pj+DgYKZPn869995LXFwc27dvt9gJWr1eT0BAAL169WLFihX06NGDadOmsXv3bj777DOLHLKsXr2a8vJyWrduzdChQ8nNzeWDDz6gZcuW3H///YwZM0a5Sb379ddfGTVqFGPHjuXDDz/kyJEjrFu3jpKSEk6cOMHGjRvNevI+JSWF2NhY7OzsmDFjBseOHWP37t0YDAYGDx7M4MGDCQ4OZsCAARw5coStW7da5LNpbGxi5DVu3Djmzp3LO++8w8KFC5XdZhUUFMT48eMJCQkhMTGR0tJSLl68yIQJEwgODuazzz5TbmI2y5cvZ+vWrQBcvHiRlStX8sILL2BnZ0dFRYWy3CxOnDhBTk4ORUVFeHh4sHHjRubPn8+CBQuIjY1VlptFRUUFHh4ePP3001RVVaHT6VixYgVTp05l7dq1LF26VLlJverRowezZs0yfv/2228zZ84c3n77bXbt2kVubi4PPfQQ48aNo127dpw+fdpke1E/bCK89Hq98YRsfn6+stsiQkNDmTp1KsXFxdx1111w7UTxpUuXlKVmcf1Kq7u7O1xbRqhDhw4AtG/fnpqaP7Aezx+QnJzM4sWLUavVHDhwgKtXr+Lh4WHRkYWrqytXr15ly5YtuLq6mqzOq9Fo+PHHH03qza2kpARXV1fs7OwoKyujoKAAT09PADw9PSkqKlJuIuqBTYRXq1atKC8vB6Br167KbrMyGAyMHz+eKVOm0L59e1q0aGEMrNzcXItNmjUYDJSUlLBjxw4yMzPp0qULMTExAHz55ZdoNBrlJmbh5+dH8+bN8fb2pri4GG9vb3Jzc9Hr9RZ7UlReXh6vvvoqY8eOpXfv3tjb2xunrpSXl9O/f3/lJmbVunVrLl26hE6nw93dHR8fH9LT0wHIzMykTZs2yk1EPbCJ8NqzZw9jxoxhyJAhFj9sXL16NUeOHGHKlCnMnz8fjUbDqFGjCAgIID09nYCAAOUmZnHPPfcwevRoRo0aha+vL05OTpw8eZIBAwbw+uuvW+y82yuvvEJgYCDR0dEMHz6ckJAQFixYwLBhw5gxY4ay3Czat29PVFQUgYGBlJeXY2dnx/bt2xk0aBCvvvoqf//735Wb1Kvs7Gz69+/PunXreOONNwgPD+fNN99k+PDhvPXWW3h4eODu7k5AQAAPPvhgg0+s/l9lEyfshRBCySZGXkIIoSThJYSwSRJeQgibJOHVyOzfv1/ZZCIsLAydTqdsNhEREaFsEsLiJLys2NWrV3n22WdZvXq1yaTIPyMnJ0fZZGLOnDl13niufDCwEA1BrjZasezsbP7zn/+YXPpfvXo1KpWKffv2ERMTw9atW0lKSsLT0xO1Wo2rqyvbt2/n6NGjDB06lL/+9a9UV1fz7LPP0r9/f9asWcP48eN5/fXX8fPz4/vvv+fdd981TjINCAggJiaGgIAAhg4dSnJyMosWLUKn07Fq1Spat25NSkoKX3zxBXPnzsXDw4PU1FT+9a9/sWDBAqZNm8ayZctYuHAhrVq1uuF3I0T9kpGXFWvfvj1OTk5MnjzZuODfY489Rl5eHvb29lRVVQEwY8YM5s6dy/bt25k0aRKdO3emuLgYgFmzZhEREcG8efOM+920aRNlZWV8//33HDx4kMLCQmPfdTU1NUybNo0pU6Zw+vRp3n//fRYsWMCiRYtwdHTk8uXLxMXFkZubS2FhIQcPHmTJkiXG+XgSXMLcJLysmEajYcyYMaxZswZvb290Oh1BQUFMnz6dgQMHGu86cHV1BcDd3R2VSoWTk5Nxtvv124aaNGli3K+joyMTJkxg2bJlpKSk0Lx5c2PfjTUajYYmTZoYb8+qrq7GYDBQXV2NRqOhR48eLFu2jB07dvDcc8+Rk5NDq1atyMrKUu5OiHon4WXFzp8/zyuvvMLLL7/MxYsXsbOz4+GHHyY0NJTz588ry29p4cKFBAUFsXLlSmNbcHAwW7ZsYfLkycyaNeuO7oucPn0606dPZ8qUKTg4OODu7k7Hjh157bXXCA0NJTs7m/DwcDZv3kx1dTXJycnKXQhRr+Sc1/+wgIAAoqKijCMzIf6XyMjrf9hrr73W4GvtC2EuMvISQtgkGXkJIWyShJcQwiZJeAkhbJKElxDCJv1/WMCFwk3GHPQAAAAASUVORK5CYII="
    }
   },
   "cell_type": "markdown",
   "id": "ec7d2feb",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7bc14d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the stratified k fold instance\n",
    "tscv = TimeSeriesSplit(n_splits=NUMBER_OF_FOLDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eea75f6",
   "metadata": {},
   "source": [
    "## TRAINING WITH CROSS VALIDATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843923de",
   "metadata": {},
   "source": [
    "### OTM Hyperparams Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "683b3896",
   "metadata": {},
   "outputs": [],
   "source": [
    "def defining_hyperparams(trial, hyperparam_ranges):\n",
    "    v1 = trial.suggest_int('max_depth', hyperparam_ranges['max_depth'][0], hyperparam_ranges['max_depth'][1], log=hyperparam_ranges['max_depth'][2])\n",
    "    v2 = trial.suggest_int('n_estimators', hyperparam_ranges['n_estimators'][0], hyperparam_ranges['n_estimators'][1], log=hyperparam_ranges['n_estimators'][2])\n",
    "    v3 = trial.suggest_float('learning_rate', hyperparam_ranges['learning_rate'][0], hyperparam_ranges['learning_rate'][1], log=hyperparam_ranges['learning_rate'][2])\n",
    "    v4 = trial.suggest_float('reg_alpha', hyperparam_ranges['reg_alpha'][0], hyperparam_ranges['reg_alpha'][1], log=hyperparam_ranges['reg_alpha'][2])\n",
    "    v5 = trial.suggest_float('reg_lambda', hyperparam_ranges['reg_lambda'][0], hyperparam_ranges['reg_lambda'][1], log=hyperparam_ranges['reg_lambda'][2])\n",
    "    v6 = trial.suggest_float('min_child_weight', hyperparam_ranges['min_child_weight'][0], hyperparam_ranges['min_child_weight'][1], log=hyperparam_ranges['min_child_weight'][2])\n",
    "    v7 = trial.suggest_float('subsample', hyperparam_ranges['subsample'][0], hyperparam_ranges['subsample'][1], log=hyperparam_ranges['subsample'][2])\n",
    "    v8 = trial.suggest_float('colsample_bytree', hyperparam_ranges['colsample_bytree'][0], hyperparam_ranges['colsample_bytree'][1], log=hyperparam_ranges['colsample_bytree'][2])\n",
    "    return {'max_depth':v1, 'n_estimators':v2, 'learning_rate':v3, 'reg_alpha':v4,\n",
    "            'reg_lambda':v5, 'min_child_weight':v6, 'subsample':v7, 'colsample_bytree':v8}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65a57e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-17 22:27:46,269] A new study created in memory with name: best_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 1] Training fold: 1132614 samples\n",
      "[Fold 1] Creating the preprocessing and modeling pipeline.\n",
      "[Fold 1] Fitting the model to the data.\n",
      "[Fold 1] WMAPE Training:   57.84899365922701.\n",
      "[Fold 1] WMAPE Validation: 55.3199765377801.\n",
      "[Fold 2] Training fold: 2265226 samples\n",
      "[Fold 2] Creating the preprocessing and modeling pipeline.\n",
      "[Fold 2] Fitting the model to the data.\n",
      "[Fold 2] WMAPE Training:   55.84037485299793.\n",
      "[Fold 2] WMAPE Validation: 54.39658841180768.\n",
      "[Fold 3] Training fold: 3397838 samples\n",
      "[Fold 3] Creating the preprocessing and modeling pipeline.\n",
      "[Fold 3] Fitting the model to the data.\n",
      "[Fold 3] WMAPE Training:   55.22954897634181.\n",
      "[Fold 3] WMAPE Validation: 83.68459679436798.\n",
      "[Fold 4] Training fold: 4530450 samples\n",
      "[Fold 4] Creating the preprocessing and modeling pipeline.\n",
      "[Fold 4] Fitting the model to the data.\n",
      "[Fold 4] WMAPE Training:   63.12182870357469.\n",
      "[Fold 4] WMAPE Validation: 112.74673179495296.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-17 22:41:31,971] Trial 0 finished with values: {'score_validation': 76.53697338472718, 'score_overfitting': 18.52678683669182} and parameters: {'max_depth': 4, 'n_estimators': 1874, 'learning_rate': 0.07408442859463599, 'reg_alpha': 3.246628584413869, 'reg_lambda': 1.3853417157368884, 'min_child_weight': 55.91921999058458, 'subsample': 0.5183597010633609, 'colsample_bytree': 0.3759645367517631}. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 1] Training fold: 1132614 samples\n",
      "[Fold 1] Creating the preprocessing and modeling pipeline.\n",
      "[Fold 1] Fitting the model to the data.\n",
      "[Fold 1] WMAPE Training:   56.688612015051945.\n",
      "[Fold 1] WMAPE Validation: 54.09014292715012.\n",
      "[Fold 2] Training fold: 2265226 samples\n",
      "[Fold 2] Creating the preprocessing and modeling pipeline.\n",
      "[Fold 2] Fitting the model to the data.\n",
      "[Fold 2] WMAPE Training:   54.76536783665471.\n",
      "[Fold 2] WMAPE Validation: 53.22509361676967.\n",
      "[Fold 3] Training fold: 3397838 samples\n",
      "[Fold 3] Creating the preprocessing and modeling pipeline.\n",
      "[Fold 3] Fitting the model to the data.\n",
      "[Fold 3] WMAPE Training:   54.17248890363147.\n",
      "[Fold 3] WMAPE Validation: 80.83611471511854.\n",
      "[Fold 4] Training fold: 4530450 samples\n",
      "[Fold 4] Creating the preprocessing and modeling pipeline.\n",
      "[Fold 4] Fitting the model to the data.\n",
      "[Fold 4] WMAPE Training:   62.39974122447958.\n",
      "[Fold 4] WMAPE Validation: 101.26715006658213.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-17 22:58:26,126] Trial 1 finished with values: {'score_validation': 72.35462533140512, 'score_overfitting': 15.34807283645069} and parameters: {'max_depth': 6, 'n_estimators': 1910, 'learning_rate': 0.020495557154895296, 'reg_alpha': 0.015835940636424795, 'reg_lambda': 4.000273119026652, 'min_child_weight': 175.07932230800267, 'subsample': 0.9208061080469275, 'colsample_bytree': 0.3528881980258338}. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 1] Training fold: 1132614 samples\n",
      "[Fold 1] Creating the preprocessing and modeling pipeline.\n",
      "[Fold 1] Fitting the model to the data.\n",
      "[Fold 1] WMAPE Training:   59.42219105941541.\n",
      "[Fold 1] WMAPE Validation: 56.411870205689375.\n",
      "[Fold 2] Training fold: 2265226 samples\n",
      "[Fold 2] Creating the preprocessing and modeling pipeline.\n",
      "[Fold 2] Fitting the model to the data.\n",
      "[Fold 2] WMAPE Training:   57.34924765330969.\n",
      "[Fold 2] WMAPE Validation: 55.559297078607685.\n",
      "[Fold 3] Training fold: 3397838 samples\n",
      "[Fold 3] Creating the preprocessing and modeling pipeline.\n",
      "[Fold 3] Fitting the model to the data.\n",
      "[Fold 3] WMAPE Training:   56.53993983093106.\n",
      "[Fold 3] WMAPE Validation: 80.8725754308733.\n",
      "[Fold 4] Training fold: 4530450 samples\n",
      "[Fold 4] Creating the preprocessing and modeling pipeline.\n",
      "[Fold 4] Fitting the model to the data.\n",
      "[Fold 4] WMAPE Training:   64.33034808369621.\n",
      "[Fold 4] WMAPE Validation: 116.29795673314966.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-17 23:09:31,170] Trial 2 finished with values: {'score_validation': 77.28542486208, 'score_overfitting': 17.874993205241914} and parameters: {'max_depth': 3, 'n_estimators': 1328, 'learning_rate': 0.05473509793044019, 'reg_alpha': 0.31788497628306767, 'reg_lambda': 0.034620487603676864, 'min_child_weight': 78.33323374532861, 'subsample': 0.4765260187225597, 'colsample_bytree': 0.5475767188391575}. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 1] Training fold: 1132614 samples\n",
      "[Fold 1] Creating the preprocessing and modeling pipeline.\n",
      "[Fold 1] Fitting the model to the data.\n",
      "[Fold 1] WMAPE Training:   61.96410613760746.\n",
      "[Fold 1] WMAPE Validation: 58.62833949213536.\n",
      "[Fold 2] Training fold: 2265226 samples\n",
      "[Fold 2] Creating the preprocessing and modeling pipeline.\n",
      "[Fold 2] Fitting the model to the data.\n",
      "[Fold 2] WMAPE Training:   59.8164176498575.\n",
      "[Fold 2] WMAPE Validation: 58.317009126524134.\n",
      "[Fold 3] Training fold: 3397838 samples\n",
      "[Fold 3] Creating the preprocessing and modeling pipeline.\n",
      "[Fold 3] Fitting the model to the data.\n",
      "[Fold 3] WMAPE Training:   59.044910257114466.\n",
      "[Fold 3] WMAPE Validation: 81.81179468192707.\n",
      "[Fold 4] Training fold: 4530450 samples\n",
      "[Fold 4] Creating the preprocessing and modeling pipeline.\n",
      "[Fold 4] Fitting the model to the data.\n",
      "[Fold 4] WMAPE Training:   66.72899225146426.\n",
      "[Fold 4] WMAPE Validation: 108.37378417336818.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-17 23:19:58,455] Trial 3 finished with values: {'score_validation': 76.78273186848868, 'score_overfitting': 14.894125294477764} and parameters: {'max_depth': 2, 'n_estimators': 1273, 'learning_rate': 0.028348180656800073, 'reg_alpha': 1.4411295087471752, 'reg_lambda': 6.674320954978955, 'min_child_weight': 29.57567448842118, 'subsample': 0.33707072794950493, 'colsample_bytree': 0.6585431453167844}. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 1] Training fold: 1132614 samples\n",
      "[Fold 1] Creating the preprocessing and modeling pipeline.\n",
      "[Fold 1] Fitting the model to the data.\n",
      "[Fold 1] WMAPE Training:   61.03703692642418.\n",
      "[Fold 1] WMAPE Validation: 57.6365979807949.\n",
      "[Fold 2] Training fold: 2265226 samples\n",
      "[Fold 2] Creating the preprocessing and modeling pipeline.\n",
      "[Fold 2] Fitting the model to the data.\n",
      "[Fold 2] WMAPE Training:   59.0425265149103.\n",
      "[Fold 2] WMAPE Validation: 57.94464892834294.\n",
      "[Fold 3] Training fold: 3397838 samples\n",
      "[Fold 3] Creating the preprocessing and modeling pipeline.\n",
      "[Fold 3] Fitting the model to the data.\n",
      "[Fold 3] WMAPE Training:   58.41987483813115.\n",
      "[Fold 3] WMAPE Validation: 81.73538418877759.\n",
      "[Fold 4] Training fold: 4530450 samples\n",
      "[Fold 4] Creating the preprocessing and modeling pipeline.\n",
      "[Fold 4] Fitting the model to the data.\n",
      "[Fold 4] WMAPE Training:   66.0656812798044.\n",
      "[Fold 4] WMAPE Validation: 116.66064329855246.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-17 23:29:55,028] Trial 4 finished with values: {'score_validation': 78.49431859911698, 'score_overfitting': 17.35303870929947} and parameters: {'max_depth': 3, 'n_estimators': 914, 'learning_rate': 0.010076401890175394, 'reg_alpha': 13.61049994431397, 'reg_lambda': 1.5023611784917092, 'min_child_weight': 18.903028493904113, 'subsample': 0.3471306223600118, 'colsample_bytree': 0.8530695236672383}. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 1] Training fold: 1132614 samples\n",
      "[Fold 1] Creating the preprocessing and modeling pipeline.\n",
      "[Fold 1] Fitting the model to the data.\n",
      "[Fold 1] WMAPE Training:   62.31261336033735.\n",
      "[Fold 1] WMAPE Validation: 59.11577759060403.\n",
      "[Fold 2] Training fold: 2265226 samples\n",
      "[Fold 2] Creating the preprocessing and modeling pipeline.\n",
      "[Fold 2] Fitting the model to the data.\n",
      "[Fold 2] WMAPE Training:   59.720716082306154.\n",
      "[Fold 2] WMAPE Validation: 57.83981755291375.\n",
      "[Fold 3] Training fold: 3397838 samples\n",
      "[Fold 3] Creating the preprocessing and modeling pipeline.\n",
      "[Fold 3] Fitting the model to the data.\n",
      "[Fold 3] WMAPE Training:   58.905244204937155.\n",
      "[Fold 3] WMAPE Validation: 81.01619053179854.\n",
      "[Fold 4] Training fold: 4530450 samples\n",
      "[Fold 4] Creating the preprocessing and modeling pipeline.\n",
      "[Fold 4] Fitting the model to the data.\n",
      "[Fold 4] WMAPE Training:   66.67697114092702.\n",
      "[Fold 4] WMAPE Validation: 105.32806646271875.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-17 23:38:32,334] Trial 5 finished with values: {'score_validation': 75.82496303450877, 'score_overfitting': 13.921076837381849} and parameters: {'max_depth': 2, 'n_estimators': 645, 'learning_rate': 0.09608212941424187, 'reg_alpha': 0.2121221686964179, 'reg_lambda': 13.626026016767106, 'min_child_weight': 25.01543104990202, 'subsample': 0.4631480787904752, 'colsample_bytree': 0.31381477733617474}. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 1] Training fold: 1132614 samples\n",
      "[Fold 1] Creating the preprocessing and modeling pipeline.\n",
      "[Fold 1] Fitting the model to the data.\n",
      "[Fold 1] WMAPE Training:   57.663981962366776.\n",
      "[Fold 1] WMAPE Validation: 54.941776448865056.\n",
      "[Fold 2] Training fold: 2265226 samples\n",
      "[Fold 2] Creating the preprocessing and modeling pipeline.\n",
      "[Fold 2] Fitting the model to the data.\n",
      "[Fold 2] WMAPE Training:   55.78921742490298.\n",
      "[Fold 2] WMAPE Validation: 54.24919158509599.\n",
      "[Fold 3] Training fold: 3397838 samples\n",
      "[Fold 3] Creating the preprocessing and modeling pipeline.\n",
      "[Fold 3] Fitting the model to the data.\n",
      "[Fold 3] WMAPE Training:   55.176534143779776.\n",
      "[Fold 3] WMAPE Validation: 81.72987493114586.\n",
      "[Fold 4] Training fold: 4530450 samples\n",
      "[Fold 4] Creating the preprocessing and modeling pipeline.\n",
      "[Fold 4] Fitting the model to the data.\n",
      "[Fold 4] WMAPE Training:   63.12350828252797.\n",
      "[Fold 4] WMAPE Validation: 127.21735183645346.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-17 23:48:41,934] Trial 6 finished with values: {'score_validation': 79.53454870039009, 'score_overfitting': 21.596238246995718} and parameters: {'max_depth': 5, 'n_estimators': 718, 'learning_rate': 0.038171411674510505, 'reg_alpha': 1.0293671883307607, 'reg_lambda': 8.336070416490989, 'min_child_weight': 32.196587720642704, 'subsample': 0.435884974380148, 'colsample_bytree': 0.8891894114968563}. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 1] Training fold: 1132614 samples\n",
      "[Fold 1] Creating the preprocessing and modeling pipeline.\n",
      "[Fold 1] Fitting the model to the data.\n",
      "[Fold 1] WMAPE Training:   61.92422789733678.\n",
      "[Fold 1] WMAPE Validation: 58.62971408968004.\n",
      "[Fold 2] Training fold: 2265226 samples\n",
      "[Fold 2] Creating the preprocessing and modeling pipeline.\n",
      "[Fold 2] Fitting the model to the data.\n",
      "[Fold 2] WMAPE Training:   59.83646644146396.\n",
      "[Fold 2] WMAPE Validation: 58.61181250669648.\n",
      "[Fold 3] Training fold: 3397838 samples\n",
      "[Fold 3] Creating the preprocessing and modeling pipeline.\n",
      "[Fold 3] Fitting the model to the data.\n",
      "[Fold 3] WMAPE Training:   59.13229003498913.\n",
      "[Fold 3] WMAPE Validation: 82.68931651016543.\n",
      "[Fold 4] Training fold: 4530450 samples\n",
      "[Fold 4] Creating the preprocessing and modeling pipeline.\n",
      "[Fold 4] Fitting the model to the data.\n",
      "[Fold 4] WMAPE Training:   66.73138266568738.\n",
      "[Fold 4] WMAPE Validation: 105.92002887605287.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-17 23:56:50,661] Trial 7 finished with values: {'score_validation': 76.4627179956487, 'score_overfitting': 14.556626235779392} and parameters: {'max_depth': 3, 'n_estimators': 373, 'learning_rate': 0.015215522233919835, 'reg_alpha': 4.790042536778081, 'reg_lambda': 0.3125602017756653, 'min_child_weight': 267.83125613462795, 'subsample': 0.5721331531572124, 'colsample_bytree': 0.9000890310480502}. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 1] Training fold: 1132614 samples\n",
      "[Fold 1] Creating the preprocessing and modeling pipeline.\n",
      "[Fold 1] Fitting the model to the data.\n",
      "[Fold 1] WMAPE Training:   62.89888873384829.\n",
      "[Fold 1] WMAPE Validation: 59.64089468971265.\n",
      "[Fold 2] Training fold: 2265226 samples\n",
      "[Fold 2] Creating the preprocessing and modeling pipeline.\n",
      "[Fold 2] Fitting the model to the data.\n",
      "[Fold 2] WMAPE Training:   60.76829907638882.\n",
      "[Fold 2] WMAPE Validation: 59.109812148887244.\n",
      "[Fold 3] Training fold: 3397838 samples\n",
      "[Fold 3] Creating the preprocessing and modeling pipeline.\n",
      "[Fold 3] Fitting the model to the data.\n",
      "[Fold 3] WMAPE Training:   59.82530368920007.\n",
      "[Fold 3] WMAPE Validation: 81.1865638349941.\n",
      "[Fold 4] Training fold: 4530450 samples\n",
      "[Fold 4] Creating the preprocessing and modeling pipeline.\n",
      "[Fold 4] Fitting the model to the data.\n",
      "[Fold 4] WMAPE Training:   67.3340808472215.\n",
      "[Fold 4] WMAPE Validation: 107.27829953131331.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-18 00:07:02,766] Trial 8 finished with values: {'score_validation': 76.80389255122682, 'score_overfitting': 14.097249464562157} and parameters: {'max_depth': 2, 'n_estimators': 1094, 'learning_rate': 0.02466295963779933, 'reg_alpha': 0.3150036162502211, 'reg_lambda': 11.012646960637968, 'min_child_weight': 52.20445490881524, 'subsample': 0.6461287364778524, 'colsample_bytree': 0.312352825682646}. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 1] Training fold: 1132614 samples\n",
      "[Fold 1] Creating the preprocessing and modeling pipeline.\n",
      "[Fold 1] Fitting the model to the data.\n",
      "[Fold 1] WMAPE Training:   58.926660189915104.\n",
      "[Fold 1] WMAPE Validation: 55.73331248143224.\n",
      "[Fold 2] Training fold: 2265226 samples\n",
      "[Fold 2] Creating the preprocessing and modeling pipeline.\n",
      "[Fold 2] Fitting the model to the data.\n",
      "[Fold 2] WMAPE Training:   56.987568212513665.\n",
      "[Fold 2] WMAPE Validation: 55.08021829145595.\n",
      "[Fold 3] Training fold: 3397838 samples\n",
      "[Fold 3] Creating the preprocessing and modeling pipeline.\n",
      "[Fold 3] Fitting the model to the data.\n",
      "[Fold 3] WMAPE Training:   56.48762153838687.\n",
      "[Fold 3] WMAPE Validation: 80.43343725236954.\n",
      "[Fold 4] Training fold: 4530450 samples\n",
      "[Fold 4] Creating the preprocessing and modeling pipeline.\n",
      "[Fold 4] Fitting the model to the data.\n",
      "[Fold 4] WMAPE Training:   64.27590316772189.\n",
      "[Fold 4] WMAPE Validation: 124.00488496223532.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-09-18 00:17:08,025] Trial 9 finished with values: {'score_validation': 78.81296324687327, 'score_overfitting': 19.643524969738884} and parameters: {'max_depth': 4, 'n_estimators': 747, 'learning_rate': 0.021169926977976938, 'reg_alpha': 0.1293796032285937, 'reg_lambda': 0.7602866212439027, 'min_child_weight': 16.218681371750826, 'subsample': 0.9271468821147466, 'colsample_bytree': 0.7344368193922536}. \n"
     ]
    }
   ],
   "source": [
    "# Configuring and execute the study\n",
    "study = optuna.create_study(study_name='best_model', directions=[OPTUNA_PARAMS['DIRECTION'], OPTUNA_PARAMS['DIRECTION']])\n",
    "study.set_metric_names([\"score_validation\", \"score_overfitting\"])\n",
    "\n",
    "study.optimize(utils.create_objective_function(hyperparam_ranges            = MODEL_RANGE_PARAMS,\n",
    "                                            tscv                            = tscv,\n",
    "                                            x_data                          = [X_train,y_train],\n",
    "                                            regressor_model                 = XGBRegressor,\n",
    "                                            columns_to_use                  = [FEATURE_COLUMNS, LABEL_COLUMNS, NUMERIC_COLUMNS, CATEGORICAL_COLUMNS],\n",
    "                                            eval_features                   = EVAL_FEATURES,\n",
    "                                            preprocessing                   = PREPROCESSING,\n",
    "                                            random_state                    = RANDOM_STATE,\n",
    "                                            defining_hyperparams_function   = defining_hyperparams,\n",
    "                                            multi_scores                    = True),\n",
    "                                            n_trials                        = OPTUNA_PARAMS['TRIALS']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb5e8bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 6, 'n_estimators': 1910, 'learning_rate': 0.020495557154895296, 'reg_alpha': 0.015835940636424795, 'reg_lambda': 4.000273119026652, 'min_child_weight': 175.07932230800267, 'subsample': 0.9208061080469275, 'colsample_bytree': 0.3528881980258338}\n"
     ]
    }
   ],
   "source": [
    "# saving best params of otm optuna into variable\n",
    "_, best_params = utils.get_best_params(study)\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522bf724",
   "metadata": {},
   "source": [
    "## TRAINING WITH BEST PARAMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca70d614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe with the predictions and expected values\n",
    "df_results = pd.DataFrame()\n",
    "\n",
    "# dataframe to hold important features of models\n",
    "df_feature_importances = pd.DataFrame()\n",
    "\n",
    "# dictionary to hold the cross validation results from each fold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523dee4c",
   "metadata": {},
   "source": [
    "## FITTING THE MODEL THE LAST TIME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcdf0aa",
   "metadata": {},
   "source": [
    "### Re-creating the pre-processing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6976d4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_preproc = utils.create_preprocessing_pipeline(preprocessing_configs=PREPROCESSING, numeric_columns=NUMERIC_COLUMNS, categorical_columns=CATEGORICAL_COLUMNS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8106e44",
   "metadata": {},
   "source": [
    "### Fitting the model  to the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f58c5899",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m Pipeline(steps\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m      2\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreprocessor\u001b[39m\u001b[38;5;124m'\u001b[39m, pipe_preproc), (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m, XGBRegressor(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbest_params, random_state\u001b[38;5;241m=\u001b[39mRANDOM_STATE))\n\u001b[0;32m      3\u001b[0m ])\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# training the model\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mFEATURE_COLUMNS\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLABEL_COLUMNS\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mravel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\yuric\\OneDrive\\Área de Trabalho\\HACKATON\\CP007--Hackathon-Forecast-2025--Big-Data\\.venv\\lib\\site-packages\\sklearn\\base.py:1365\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1358\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1360\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1361\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1362\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1363\u001b[0m     )\n\u001b[0;32m   1364\u001b[0m ):\n\u001b[1;32m-> 1365\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\yuric\\OneDrive\\Área de Trabalho\\HACKATON\\CP007--Hackathon-Forecast-2025--Big-Data\\.venv\\lib\\site-packages\\sklearn\\pipeline.py:663\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    657\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    658\u001b[0m         last_step_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_metadata_for_step(\n\u001b[0;32m    659\u001b[0m             step_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m    660\u001b[0m             step_params\u001b[38;5;241m=\u001b[39mrouted_params[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]],\n\u001b[0;32m    661\u001b[0m             all_params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[0;32m    662\u001b[0m         )\n\u001b[1;32m--> 663\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator\u001b[38;5;241m.\u001b[39mfit(Xt, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mlast_step_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    665\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\yuric\\OneDrive\\Área de Trabalho\\HACKATON\\CP007--Hackathon-Forecast-2025--Big-Data\\.venv\\lib\\site-packages\\xgboost\\core.py:729\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    727\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    728\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 729\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\yuric\\OneDrive\\Área de Trabalho\\HACKATON\\CP007--Hackathon-Forecast-2025--Big-Data\\.venv\\lib\\site-packages\\xgboost\\sklearn.py:1247\u001b[0m, in \u001b[0;36mXGBModel.fit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[0m\n\u001b[0;32m   1244\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1245\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1247\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1248\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1249\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1250\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_num_boosting_rounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1251\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1256\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1258\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1261\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_evaluation_result(evals_result)\n\u001b[0;32m   1262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\yuric\\OneDrive\\Área de Trabalho\\HACKATON\\CP007--Hackathon-Forecast-2025--Big-Data\\.venv\\lib\\site-packages\\xgboost\\core.py:729\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    727\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    728\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 729\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\yuric\\OneDrive\\Área de Trabalho\\HACKATON\\CP007--Hackathon-Forecast-2025--Big-Data\\.venv\\lib\\site-packages\\xgboost\\training.py:183\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m--> 183\u001b[0m \u001b[43mbst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miteration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\yuric\\OneDrive\\Área de Trabalho\\HACKATON\\CP007--Hackathon-Forecast-2025--Big-Data\\.venv\\lib\\site-packages\\xgboost\\core.py:2247\u001b[0m, in \u001b[0;36mBooster.update\u001b[1;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[0;32m   2243\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_dmatrix_features(dtrain)\n\u001b[0;32m   2245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2246\u001b[0m     _check_call(\n\u001b[1;32m-> 2247\u001b[0m         \u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXGBoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2248\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\n\u001b[0;32m   2249\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2250\u001b[0m     )\n\u001b[0;32m   2251\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2252\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(dtrain, output_margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', pipe_preproc), ('model', XGBRegressor(**best_params, random_state=RANDOM_STATE))\n",
    "])\n",
    "\n",
    "# training the model\n",
    "pipeline.fit(X_train.loc[:, FEATURE_COLUMNS], y_train.loc[:, LABEL_COLUMNS].values.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da353091",
   "metadata": {},
   "source": [
    "### Evaluating model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75849b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a reference copy of the training and test data\n",
    "X_train = df.loc[X_train.index].copy()\n",
    "X_test = df.loc[X_test.index].copy()\n",
    "train, test = X_train.loc[:, EVAL_FEATURES + [LABEL_COLUMNS]], X_test.loc[:, EVAL_FEATURES + [LABEL_COLUMNS]]\n",
    "\n",
    "# getting the predictions for the training and test sets\n",
    "train['predicted'] = pipeline.predict(X_train.loc[:, FEATURE_COLUMNS])\n",
    "test['predicted'] = pipeline.predict(X_test.loc[:, FEATURE_COLUMNS])\n",
    "\n",
    "# rounding train and test predictions\n",
    "train['predicted_rounded'] = np.round(train['predicted'].values).astype(np.int32)\n",
    "test['predicted_rounded'] = np.round(test['predicted'].values).astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282434f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the error for the training and test sets\n",
    "error_train = utils.wmape_score(y_true = train[LABEL_COLUMNS].values, y_pred = train['predicted'].values)\n",
    "error_test = utils.wmape_score(y_true = test[LABEL_COLUMNS].values, y_pred = test['predicted'].values)\n",
    "\n",
    "error_train_rounded = utils.wmape_score(y_true = train[LABEL_COLUMNS].values, y_pred = train['predicted_rounded'].values)\n",
    "error_test_rounded = utils.wmape_score(y_true = test[LABEL_COLUMNS].values, y_pred = test['predicted_rounded'].values)\n",
    "\n",
    "print(f\"WMAPE Train: {error_train:.3f}%\")\n",
    "print(f\"WMAPE Test: {error_test:.3f}%\")\n",
    "\n",
    "print(f\"WMAPE Train Rounded: {error_train_rounded:.3f}%\")\n",
    "print(f\"WMAPE Test Rounded: {error_test_rounded:.3f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4806b9e5",
   "metadata": {},
   "source": [
    "## SAVING RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e27259e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_directory = f\"outputs/{dt.datetime.now().strftime(format= '%Y%m%d%H%M')}_XGBOOST/\"\n",
    "output_model_directory = output_directory + 'model'\n",
    "output_data_directory = output_directory + 'data'\n",
    "output_params_directory = output_directory + 'params'\n",
    "output_optuna_directory = output_directory + 'study'\n",
    "\n",
    "for directory in [output_model_directory, output_data_directory, output_params_directory, output_params_directory, output_optuna_directory]:\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72df1430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the json dictionary with the parameters\n",
    "with open(f'{output_params_directory}/exepriment_params.json', 'w') as f:\n",
    "    configs_all = {**configs, **best_params}\n",
    "    json.dump(configs_all, f, indent=4)\n",
    "\n",
    "# datasets\n",
    "X_train.to_parquet(f'{output_data_directory}/training_set.parquet', index=False)\n",
    "X_test.to_parquet(f'{output_data_directory}/test_set.parquet', index=False)\n",
    "\n",
    "# pipeline\n",
    "with open(f'{output_model_directory}/pipeline.pkl', 'wb') as f:\n",
    "    pickle.dump(pipeline, f)\n",
    "\n",
    "# saving study\n",
    "with open(f'{output_optuna_directory}/best_metric_study.pkl', 'wb') as f:\n",
    "    pickle.dump(study, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
