# ğŸš€ Guia de ExecuÃ§Ã£o Completo - PrevisÃ£o de Vendas

Este guia fornece instruÃ§Ãµes detalhadas para executar todo o pipeline de previsÃ£o de vendas, desde a configuraÃ§Ã£o inicial atÃ© a obtenÃ§Ã£o dos resultados finais.

## ğŸ“‹ PrÃ©-requisitos

### Sistema Operacional
- Windows 10/11, macOS 10.15+, ou Ubuntu 18.04+
- Pelo menos 8GB de RAM (16GB recomendado)
- 10GB de espaÃ§o livre em disco

### Software NecessÃ¡rio
- **Python 3.8+** (recomendado: 3.9 ou 3.10)
- **Git** para controle de versÃ£o
- **Jupyter Notebook** ou **VS Code com extensÃ£o Python**

### Hardware Opcional
- **GPU NVIDIA** com drivers CUDA para aceleraÃ§Ã£o (opcional)

## ğŸ› ï¸ ConfiguraÃ§Ã£o do Ambiente

### Passo 1: Clonar o RepositÃ³rio

```bash
# Clone o repositÃ³rio
git clone https://github.com/gabrielmotablima/CP007--Hackathon-Forecast-2025--Big-Data.git

# Entre no diretÃ³rio
cd CP007--Hackathon-Forecast-2025--Big-Data
```

### Passo 2: Criar Ambiente Virtual

```bash
# Criar ambiente virtual
python -m venv forecast_env

# Ativar ambiente (Windows)
forecast_env\Scripts\activate

# Ativar ambiente (Linux/Mac)
source forecast_env/bin/activate
```

### Passo 3: Instalar DependÃªncias

```bash
# Atualizar pip
python -m pip install --upgrade pip

# Instalar dependÃªncias principais
pip install -r requirements.txt

# Para suporte GPU (opcional)
pip install xgboost[gpu]
```

### Passo 4: Verificar InstalaÃ§Ã£o

```bash
# Verificar Python e bibliotecas
python -c "import pandas, numpy, xgboost, sklearn, optuna; print('âœ… Todas as bibliotecas instaladas com sucesso!')"
```

## ğŸ“‚ PreparaÃ§Ã£o dos Dados

### Passo 1: Estrutura de DiretÃ³rios

Certifique-se de que a estrutura esteja organizada:

```
CP007--Hackathon-Forecast-2025--Big-Data/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                    # Dados brutos (vocÃª deve colocar aqui)
â”‚   â”‚   â”œâ”€â”€ part-00000-tid-2779033056155408584-*.parquet  # Dados de lojas
â”‚   â”‚   â”œâ”€â”€ part-00000-tid-5196563791502273604-*.parquet  # Dados de transaÃ§Ãµes
â”‚   â”‚   â”œâ”€â”€ part-00000-tid-7173294866425216458-*.parquet  # Dados de produtos
â”‚   â”‚   â”œâ”€â”€ georef-zipcode.csv                            # Dados de CEP
â”‚   â”‚   â””â”€â”€ processed_usa_holiday.csv                     # Feriados
â”‚   â””â”€â”€ processed/              # Dados processados (serÃ¡ criado)
â””â”€â”€ notebooks/                  # Notebooks de execuÃ§Ã£o
```

### Passo 2: Colocar os Dados

1. **Obtenha os arquivos de dados** (conforme instruÃ§Ãµes do hackathon)
2. **Coloque todos os arquivos** no diretÃ³rio `data/raw/`
3. **Verifique os nomes** dos arquivos conforme esperado pelos notebooks

## ğŸ”„ ExecuÃ§Ã£o do Pipeline

### Etapa 1: PreparaÃ§Ã£o dos Dados (OBRIGATÃ“RIA)

```bash
# Iniciar Jupyter Notebook
jupyter notebook

# Ou usar VS Code
code notebooks/01_data_preparation/01_prepare_data.ipynb
```

**Executar o notebook completo:**
1. Abra `notebooks/01_data_preparation/01_prepare_data.ipynb`
2. Execute todas as cÃ©lulas sequencialmente (`Kernel â†’ Restart & Run All`)
3. Aguarde conclusÃ£o (pode levar 10-30 minutos dependendo do tamanho dos dados)
4. Verifique se foi criado: `data/processed/processed_data.parquet`

**Indicadores de sucesso:**
- âœ… Arquivo `processed_data.parquet` criado
- âœ… Sem erros nas cÃ©lulas de execuÃ§Ã£o
- âœ… Mensagens de log indicando conclusÃ£o

### Etapa 2: Treinamento do Modelo (PRINCIPAL)

```bash
# Abrir notebook de modelagem
code notebooks/02_models/01_xgboost.ipynb
```

**Executar passo a passo:**

1. **ConfiguraÃ§Ã£o e Carregamento (CÃ©lulas 1-5)**
   ```python
   # Verifique se carregou corretamente
   print(f"Dataset shape: {df.shape}")
   print(f"Colunas: {df.columns.tolist()}")
   ```

2. **DivisÃ£o dos Dados (CÃ©lulas 6-8)**
   ```python
   # Verifique as divisÃµes
   print(f"Treino: {X_train.shape}")
   print(f"Teste: {X_test.shape}")
   ```

3. **OtimizaÃ§Ã£o de HiperparÃ¢metros (CÃ©lulas 9-12)**
   - âš ï¸ **IMPORTANTE**: Esta etapa pode levar 1-3 horas
   - Monitore o progresso atravÃ©s dos logs do Optuna
   - VocÃª pode ajustar `n_trials` para reduzir tempo

4. **Treinamento Final (CÃ©lulas 13-15)**
   ```python
   # Verifique mÃ©tricas de treino
   print(f"WMAPE ValidaÃ§Ã£o: {wmape_cv}")
   print(f"WMAPE Teste: {wmape_test}")
   ```

5. **AvaliaÃ§Ã£o e ExportaÃ§Ã£o (CÃ©lulas 16-18)**
   - Gere grÃ¡ficos de performance
   - Exporte modelo treinado
   - Calcule feature importance

### Etapa 3: VersÃ£o GPU (OPCIONAL)

Se vocÃª tem GPU NVIDIA disponÃ­vel:

```bash
# Abrir versÃ£o GPU
code notebooks/02_models/02_xgboost_gpu.ipynb
```

**Vantagens da versÃ£o GPU:**
- ğŸš€ Treinamento 5-10x mais rÃ¡pido
- âš¡ OtimizaÃ§Ã£o de hiperparÃ¢metros acelerada
- ğŸ’» Melhor uso de recursos de hardware

## ğŸ“Š InterpretaÃ§Ã£o dos Resultados

### MÃ©tricas Principais

**WMAPE (Weighted Mean Absolute Percentage Error)**
- `< 10%`: Excelente performance
- `10-20%`: Boa performance 
- `20-30%`: Performance aceitÃ¡vel
- `> 30%`: Necessita melhorias

### Arquivos de SaÃ­da

ApÃ³s execuÃ§Ã£o completa, vocÃª terÃ¡:

```
data/processed/
â”œâ”€â”€ processed_data.parquet          # Dados limpos
â”œâ”€â”€ model_xgboost.pkl              # Modelo treinado
â”œâ”€â”€ best_params.json               # Melhores hiperparÃ¢metros
â””â”€â”€ feature_importance.csv         # ImportÃ¢ncia das features

results/
â”œâ”€â”€ validation_results.csv         # Resultados validaÃ§Ã£o cruzada
â”œâ”€â”€ test_predictions.csv           # PrediÃ§Ãµes no conjunto teste
â””â”€â”€ performance_plots.png          # GrÃ¡ficos de performance
```

## ğŸ”§ SoluÃ§Ã£o de Problemas

### Erros Comuns

**1. ModuleNotFoundError**
```bash
# SoluÃ§Ã£o: Reinstalar dependÃªncias
pip install -r requirements.txt --force-reinstall
```

**2. Arquivo nÃ£o encontrado**
```bash
# SoluÃ§Ã£o: Verificar estrutura de diretÃ³rios
ls data/raw/  # Deve mostrar todos os arquivos .parquet e .csv
```

**3. MemÃ³ria insuficiente**
```python
# SoluÃ§Ã£o: Reduzir tamanho do dataset temporariamente
df_sample = df.sample(frac=0.1)  # Usar 10% dos dados
```

**4. GPU nÃ£o reconhecida**
```bash
# Verificar instalaÃ§Ã£o CUDA
python -c "import xgboost as xgb; print(xgb.XGBRegressor(tree_method='gpu_hist'))"
```

### OtimizaÃ§Ãµes de Performance

**1. Reduzir tempo de execuÃ§Ã£o:**
```yaml
# Ajustar em model_params.yml
OPTUNA_PARAMS:
  n_trials: 50  # Reduzir de 100 para 50
```

**2. Usar menos folds:**
```yaml
NUMBER_OF_FOLDS: 3  # Reduzir de 4 para 3
```

**3. Limitar features:**
```yaml
FEATURES: ['internal_store_id', 'internal_product_id', 'month', 'week_of_year']
```

## ğŸ“ˆ PrÃ³ximos Passos

### ApÃ³s ExecuÃ§Ã£o Bem-sucedida

1. **AnÃ¡lise de Resultados**
   - Revisar mÃ©tricas de performance
   - Analisar feature importance
   - Identificar padrÃµes nos erros

2. **ValidaÃ§Ã£o de NegÃ³cio**
   - Comparar com dados histÃ³ricos
   - Validar com especialistas de domÃ­nio
   - Testar em cenÃ¡rios especÃ­ficos

3. **Deployment (Opcional)**
   - Criar API de prediÃ§Ã£o
   - Implementar monitoramento
   - Automatizar retreinamento

### Melhorias PossÃ­veis

1. **Feature Engineering AvanÃ§ada**
   - Features de lag mais complexas
   - InteraÃ§Ãµes entre variÃ¡veis
   - Features de sazonalidade especÃ­ficas

2. **Modelos Ensemble**
   - Combinar XGBoost com outros algoritmos
   - Stacking de mÃºltiplos modelos
   - Voting ensemble

3. **OtimizaÃ§Ã£o Temporal**
   - Modelos especÃ­ficos por categoria
   - SegmentaÃ§Ã£o por PDV
   - Forecasting multivariado

## ğŸ“ Suporte

Em caso de dÃºvidas ou problemas:

1. **Verificar logs** dos notebooks para mensagens de erro
2. **Consultar documentaÃ§Ã£o** no README.md
3. **Revisar configuraÃ§Ãµes** em model_params.yml
4. **Abrir issue** no repositÃ³rio GitHub

---

**Tempo Estimado Total: 2-4 horas**
- ConfiguraÃ§Ã£o: 30 minutos
- PreparaÃ§Ã£o dados: 30 minutos  
- Modelagem: 1-3 horas
- AnÃ¡lise: 30 minutos

**ğŸ¯ Meta: Obter WMAPE < 15% no conjunto de teste**